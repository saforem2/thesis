\makeatletter
\def\input@path{{../}}
\makeatother
\documentclass[../main.tex]{subfiles}

\newcommand{\lfop}{\mathbf{L}_{\theta}}

\begin{document}
\section{Introduction}%
\label{sec:l2hmc_intro}
We describe a new technique for performing Hamiltonian Monte-Carlo (HMC) simulations called: `Learning to Hamiltonian
Monte Carlo' (L2HMC)~\cite{2017arXiv171109268L} which expands upon the traditional HMC by using an alternative leapfrog
integrator that is parameterized by weights in a neural network.
%
We look at applying this technique to a two-dimensional Gaussian Mixture Model and a two-dimensional $U(1)$ lattice
gauge theory, and compare our results against those obtained with traditional HMC. %chktex:13
%
Ongoing issues and potential areas for improvement are also discussed, particularly within the context of
high-performance computing and long-term goals of the lattice QCD community.
%
\section{Markov Chain Monte Carlo (MCMC)}%
\label{sec:l2hmc_mcmc}
For consistency with the original paper, we adopt much of the same notation.
%
In order to better understand the theory behind the L2HMC algorithm, we begin with a general description of Markov
Chain Monte Carlo (MCMC) methods.
%
\subsection{Markov Chains}
Generally speaking, MCMC methods are a class of algorithms that use Markov Chains to sample from a particular
probability distribution that is often too complicated to sample from directly.
%
These algorithms are of tremendous importance in a wide variety of fields and are of particular importance for
simulations in lattice quantum chromodynamics (QCD) and lattice gauge theory.
%
A Markov chain can be understood as a sequence of random variables ${\{x_1, x_2, \ldots, x_T\}}$ sampled from a
conditional probability distribution $p{(x_{t+1}|x_t)}$, with the condition that the next sample $x_{t+1}$ depends only
on the current state $x_t$ and does not depend on the history of previous states ${\{x_1, x_2, \ldots,
x_{t-1}\}}$~\cite{brooks2011handbook}.
%
The set in which the $x_i$ take values is often referred to as the state space $\mathbb{S}$ of the Markov chain.
%
For finite state spaces, the initial distribution can be associated with a vector $\lambda= {(\lambda_1, \ldots,
\lambda_n)}$ defined by
%
\begin{equation}
  p(x_1 = x_i) = \lambda_i, \quad i = 1, \ldots, n
\end{equation}
%
and the transition probabilities can be associated with a transition matrix (kernel) $K$ with elements $k_{ij}$ given
by \begin{equation}
  p(x_{t+1} = x_j | x_t = x_i) = k_{ij}, \quad i = 1, \ldots, n \,\, j = 1, \ldots n
\end{equation}
This says that the ${(i, j)}^{\mathrm{th}}$ entry of the $n^{\mathrm{th}}$ power of $K$ gives the probability of
transitioning from state $i$ to state $j$ in $n$ steps.
%
For our purposes, we are usually interested in Markov chains that have stationary transition probabilities, i.e.\ the
conditional distribution of $x_{t+1}$ given $x_t$ does not depend on $t$. Note that for $i = 1, 2, \ldots, N$,
\begin{equation}
  \sum_{j=1}^{N} p_{ij} = 1.
\end{equation}

There are two main (defining) properties of Markov chains that are relevant for our discussion, namely
\emph{stationarity} and \emph{reversibility}.
%
\begin{itemize}
  \item \textbf{Stationarity:} A sequence $\{x_1, x_2, \ldots, x_n\}$ of random elements is said to be
    \emph{stationary} if for every positive integer $k$ the distribution of the $k$-tuple
    \begin{equation*}
      (x_{t+1}, \ldots, x_{t+k})
    \end{equation*}
    does not depend on $t$. An initial distribution is said to be stationary if the Markov chain specified by the
    initial distribution and transition probability distribution is stationary. 
  \item \textbf{Reversibility:} A Markov chain is said to be reversible if its transition probability is reversible
    with respect to its initial distribution. Note that this is a stronger condition than \emph{stationarity} since if
    a Markov chain is reversible it is also stationary. This condition can be better understood by noticing that for
    any $i, k  > 0 \in \mathbb{Z}$, the distributions of $(x_{i+1}, \ldots, x_{i+k})$ and $(x_{i+k}, \ldots x_{i+1})$
    are the same.
\end{itemize}
%
Additionally, we often desire that our Markov chain is
\begin{itemize}
  \item \textbf{Irreducible:} For any state of the Markov chain, there is a positive probability of visiting all other
    states.
     % For all states $i$, $j$, there exists a $k$ such that ${(T^{k})}_{i,j} \neq 0$.
  \item \textbf{Aperiodic:} There is a positive probability that the chain remains in the current state, i.e.\ %
    $p{(x_t = x_{t-1})} > 0$.
    % For all states $i$, $j$, $\gcd\{k\,:\,{(T^{k})}_{i, j} > 0\} = 1$.
\end{itemize}
If a (finite) chain satisfies both of these properties, it will always have a unique stationary distribution.

%
% Consequently, an irreducible, aperiodic Markov chain must have a unique distribution $\pi = (\pi_1, \pi_2, \ldots,
% \pi_N)$ on the state space $\mathbb{S}$ ($\pi_i \equiv$ probability of state $i$) with the property
% that~\cite{richey2010evolution}
% \begin{equation}
%   \pi = \pi T
% \end{equation}
% %
% In which case, we say that $\pi$ is the \emph{stable distribution} of the Markov chain.
% %
% Note that because of this, if $\pi$ is the stable distribution for an irreducible, aperiodic Markov chain, then we can
% use the Markov chain to sample from $\pi$.
%
% In particular, to obtain a sample select $s_1 \in \mathbb{S}$ arbitrarily. Then for any $k > 1$, if $s_{k - 1} = i$,
% select $s_k = j$ with probability $t_{ij}$. The resulting sequence $s_1, s_2, \ldots$ has the useful property that as
% $N \rightarrow \infty$,
% \begin{equation}
%   \frac{|\{k\, :\, k \leq N \,\,\text{and}\,\, s_k = j\}|}{N} \rightarrow \pi_j
% \end{equation}
% with probability one.
%
\subsection{Metropolis-Hastings algorithm}
The purpose of the Metropolis-Hastings algorithm is to generate a collection of states according to a desired
distribution $p(x)$.
%
More formally, let $p$ be the target distribution defined up to a constant over a space $\mathcal{X}$.
%
We wish to construct a Markov Chain with stationary distribution equal to the target distribution $p$.
%
We can obtain samples by simulating a Markov process.
%
Given an initial distribution $\pi_0$ and a transition kernel $K$, we construct the following sequence of random
variables
%
\begin{equation}
  X_0 \sim \pi_0, \quad X_{t+1} \sim K(\cdot|X_t).
\end{equation}
%
As discussed previously, for $p$ to be the stationary distribution of the chain, $K$ must be irreducible and aperiodic,
and $p$ has to be a \emph{fixed point} of K. Note we can express the fixed point condition as 
%
\begin{equation}
  p(\xp) = \int K(\xp | x) p(x) dx
\end{equation}
%
The stationarity condition is often satisfied by ensuring the (stronger) \emph{detailed balance condition}
%
\begin{equation}
  p{(\xp)} K{(x|\xp)} = p{(x)} K{(\xp|x)}.
\end{equation}
%
Note that we can rewrite the detailed balance condition as 
%
\begin{equation}
  \frac{K{(\xp | x)}}{K(x | \xp)} = \frac{p(\xp)}{p(x)}
\end{equation}
%
Given a proposal distribution $q{(x^{\prime}|x)}$, we can construct a transition kernel satisfying detailed balance
using Metropolis-Hastings accept/reject rules, as outlined in Alg.~\ref{alg:metropolis_hastings}.
%
\begin{algorithm}[htpb]
  \SetKwProg{Fn}{def}{\string:}{}%
  \SetKwFunction{Range}{range}%
  \SetKwFor{For}{for}{\string:}{}%
  \SetKwIF{If}{ElseIf}{Else}{if}{:}{elif}{else:}{}%
  \SetKwFor{While}{while}{:}{fintq}%
  \AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%
  \DontPrintSemicolon%
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}%
  \caption{Metropolis-Hastings Algorithm}%
  \Input{Transition kernel (proposal distribution), $q(\xp | x)$}\;
  Initialize $x_0 \sim \pi_0$.\;
  \For{$t = 0$ \KwTo\ $N$}{%
    Sample $\xp \sim q(\cdot | x_t)$\;
    Compute the acceptance probability: 
    $A(\xp | x_t) = \min\left(1, \frac{p(\xp)q(x_t | \xp)}{p(x_t)q(\xp | x_t)}\right)$\;
    With probability $A$ accept the proposed value and set $x_{t+1} = \xp$. Otherwise set $x_{t+1} = x$.
  }%
\label{alg:metropolis_hastings}
\end{algorithm}
%
In practice, a Gaussian distribution is often used as the proposal distribution, i.e.\ $q(\xp | x) = \mathcal{N}(\xp |
x, \Sigma)$. In this case the acceptance probability reduces to
%
\begin{equation}
  A(\xp | x) = \min\left(1, \frac{p(\xp)}{p(x)}\right).
\end{equation}
%
This ``random walk'' approach is conceptually simple and can be easily implemented but performs poorly for
the high-dimensional target distributions commonly encountered in lattice gauge theory and lattice QCD.
%
% This is done using a Markov process which asymptotically reaches the unique stationary
% distribution $pi(x)$ such that $\pi(x) = p(x)$.
%
% The joint distribution of the Markov chain is then determined by
% \begin{itemize}
%   \item The marginal distribution of $x_0$, called the \emph{initial distribution}.
%   \item The conditional distribution of $x_{t+1}$ given $x_t$, called the transition probability distribution. (Note
%     that by assuming stationary transition probabilities, this is independent of $t$).
% \end{itemize}
%
%
\section{Hamiltonian Monte Carlo}%
\label{sec:l2hmc_hmc}
We can improve upon this random-walk guess and check strategy by ``guiding'' the simulation according to the systems
natural dynamics using a method known as Hamiltonian (Hybrid) Monte Carlo (HMC).
%
In HMC, model samples can be obtained by simulating a physical system governed by a Hamiltonian comprised of kinetic
and potential energy functions that govern a particles dynamics.
%
Explicitly, HMC lifts the target distribution onto a joint probability distribution in phase space $(x, v)$. 
%
A new state is then obtained by solving the equations of motion for a fixed period of time using a volume-preserving
integrator (most commonly the \emph{leapfrog integrator}).
%
The addition of random (typically normally distributed) momenta enourages long-distance jumpy in state space with a
single Metropolis-Hastings (MH) step.
%
Let the `position' of the physical state be denoted by a vector $x \in\mathbb{R}^{n}$ and the conjugate momenta of the
physical state be denoted by a vector $v \in\mathbb{R}^{n}$.  Then the Hamiltonian reads
%
\begin{align}
    \mathcal{H}(x, v) &= U(x) + K(v)\\
                      & = U(x) + \frac{1}{2} v^{T} \,v,
    \label{eq:hamiltonian}
\end{align}
%
where $U(x)$ is the potential energy, and $K(v)=\frac{1}{2}v^{T}v$ the kinetic energy.
%
We assume without loss of generality that the position and momentum variables are independently distributed.
%
That is, we assume the target distribution of the system can be written as $p(x, v) = p(x) p(v)$.
%
Further, instead of sampling $p(x)$ directly, HMC operates by sampling from the canonical distribution $p(x, v) =
\frac{1}{Z} \exp(-\mathcal{H}(x, v)) = p(x) p(v)$, for some partition function $Z$ that provides a normalization
factor.
%
Additionally, we assume the momentum is distributed according to an identity-covariance Gaussian given by $p(v) \propto
\exp{(-\frac{1}{2} v^{T} \, v)}$ For convenience, we will denote the combined state of the system by $\xi \equiv (x,
v)$.
%
From this augmented state $\xi$, HMC produces a proposed state $\xi^{\prime} = (x^{\prime}, v^{\prime})$ by
approximately integrating Hamiltonian dynamics jointly on $x$ and $v$.
%
This integration is performed along approximate iso-probability contours of $p(x, v) = p(x) p(v)$ due to the
Hamiltonians energy conservation.
%
\subsection{Hamiltonian Dynamics}
% \todo[inline]{TODO:\@ Describe current issues with traditional HMC (mixing
% between modes for GMM model, issues with using HMC in LQCD, etc.)}
%
The state $\xi$ is modified in such a way that $\mathcal{H}(\xi)$ remains constant thorughout the simulation.
%
The differential equations governing the motion through state space are given by
%
\begin{align}
    \dot x_i &= \frac{\partial \mathcal{H}}{\partial v_i} = v_i\\
    \dot v_i &= -\frac{\partial \mathcal{H}}{\partial x_i} = - \frac{\partial
        U}{\partial x_i}
\label{eq:hamiltons_equations}
\end{align}
%
It can be shown~\cite{2012arXiv1206.1901N} that the above transformation is volume-preserving and reversible, two
necessary factors to guarantee asymptotic convergence of the simulation to the target distribution.
%
The dynamics are simulated using the leapfrog integrator, which for a single time step consists of:
%
\begin{align}
    v^{\frac{1}{2}} &= v - \frac{\eps}{2} \partial_x U(x)\\
    x^{\prime} &= x + \eps v^{\frac{1}{2}}\\
    v^{\prime} &= v - \frac{\eps}{2} \partial_x U(x^{\prime}).
    \label{eq:generic_leapfrog}
\end{align}
%
We write the action of the leapfrog integrator in terms of an operator $\mathbf{L}: \mathbf{L}\xi \equiv \mathbf{L}(x,
v) \equiv (x^{\prime}, v^{\prime})$, and introduce a momentum flip operator $\mathbf{F}: \mathbf{F}(x, v) \equiv (x,
-v)$.
%
The Metropolis-Hastings acceptance probability for the HMC proposal is given by:
%
\begin{equation}
    A(\mathbf{F}\mathbf{L} \xi | \xi) = \min\left(1,
        \frac{p(\mathbf{F}\mathbf{L}\xi)}{p(\xi)}\left|
        \frac{\partial\left[\mathbf{F}\mathbf{L}\xi\right]}
        {\partial\xi^{T}}\right|\right),
\label{eq:metropolis_hastings}
\end{equation}
%
Where $\left|\frac{\partial\left[\mathbf{F}\mathbf{L}\xi\right]} {\partial\xi^{T}}\right|$ denotes the determinant of
the Jacobian describing the transformation, and is equal to $1$ for traditional HMC.\@
%
\subsubsection{Properties of Hamiltonian Dynamics}
%
There are three fundamental properties of Hamiltonian dynamics which are crucial to its use in constructing Markov
Chain Monte Carlo updates.
%
\begin{enumerate}
    \item \textbf{Reversibility:} Hamiltonian dynamics are \textit{reversible}
        --- the mapping from $\mathbf{L}: \xi(t) \rightarrow \xi^{\prime} =
        \xi(t + s)$ is one-to-one, and consequently has an inverse
        $\mathbf{L}^{-1}$, obtained by negating the time derivatives in
        Eq.~\ref{eq:hamiltons_equations}.
    \item \textbf{Conservation of the Hamiltonian:} Moreover, the dynamics
        \textit{keeps the Hamiltonian invariant}.
    \item \textbf{Volume preservation:} The final property of Hamiltonian
        dynamics is that it \textit{preserves volume} in $(x, v)$ phase space
        (i.e. Liouville's Theorem).
\end{enumerate}
%
% L2HMC {{{
\section{L2HMC}%
\label{sec:l2hmc_l2hmc}
As in the previously described HMC algorithm, we start by augmenting the current state $x \in \mathbb{R}^n$ with a
continuous momentum variable $v \in \mathbb{R}^{n}$ drawn from a standard normal distribution.
%
Additionally, we indtroduce a binary direction variable $d \in \{ -1, 1\}$, drawn from a uniform distribution. 
%
The complete augmented state is then denoted by $\xi \equiv (x, v, d)$, with probability density $p(\xi) = p(x) p(v)
p(d)$.
%
In order to improve the expressivity of our model, for each step $t$ of the leapfrog operator $\mathbf{L}_{\theta}$ we
assign a fixed random binary mask $m^{t} \in{\{0, 1\}}^n$ that will determine which variables are affected by each
sub-update.
%
The mask $m^t$ is drawn uniformly from the set of binary vectors satisfying $\sum_{i=1}^{n} m_{i}^{t} = \lfloor
\frac{n}{2}\rfloor$, i.e.\ half the entries of $m^t$ are $0$ and half are $1$.
%
Additionally, we write $\bar m^{t} = \mathbbm{1} - m^{t}$ and $x_{m^t} = x \odot m^{t}$, where $\odot$ denotes
element-wise multiplication, and $\mathbbm{1}$ the vector of $1$'s in each entry.
%
\subsection{Augmented Leapfrog}
%
We begin with a subset of the augmented space, $\zeta_1 \equiv (x, \partial_{x} U(x), t)$, independent of the momentum
$v$.
%
We introduce three new functions of $\zeta_1$: $T_v$, $Q_v$, and $S_v$.
%
We can then perform a single time-step of our modified leapfrog integrator $\mathbf{L}_{\theta}$.
%

First, we update the momentum $v$, which depends only on the subset $\zeta_1$.
%
This update is written
%
\begin{equation}
  \vp = v \odot 
    \underbrace{\exp\left(\frac{\varepsilon}{2} S_{v}(\zeta_1)\right)}_{\text{\footnotesize{Momentum scaling}}}%
    - \frac{\varepsilon}{2}\left[\partial_{x} U(x)%
    \odot\underbrace{\exp(\varepsilon Q_{v}(\zeta_1))}_{\text{\footnotesize{Gradient scaling}}}%
    + \underbrace{T_v(\zeta_1)}_{\text{\footnotesize{Translation}}}\right].
    \label{eq:update_momentum_forward1}
\end{equation}
%
and the corresponding Jacobian is given by: $\exp{\left(\frac{\eps}{2}\mathbbm{1} \cdot S_{v}(\zeta_1)\right)}$.
%
Next, we update $x$ by first updating a subset of the coordinates of $x$ (determined according to the mask $m^t$),
followed by the complementary subset (determined from $\bar m^{t}$).
%
The first update affects only $x_{m^{t}}$ and produces $x^{\prime}$.
%
This update depends only on the subset $\zeta_2 \equiv (x_{\bar m^t}, v, t)$.
%
Following this, we perform the second update which only affects $x_{\bar{m}^t}^{\prime}$ and depends only on $\zeta_3
\equiv (x_{m^t}^{\prime}, v, t)$, to produce $x^{\prime\prime}$:
%
\begin{align}
  x^{\prime} &= x_{\bar{m}^t} + m^{t}\odot\left[x \odot \exp{(\eps S_x(\zeta_2))} + \eps\left(v^{\prime}\odot%
    \exp{(\eps Q_x(\zeta_2))} + T_x(\zeta_2)\right)\right]\\
  x^{\prime\prime} &= x^{\prime}_{m^t} + \bar m^t \odot \left[x^{\prime} \odot \exp{(\eps S_x(\zeta_3))} +%
    \eps\left(v^{\prime} \odot \exp{(\eps Q_x(\zeta_3))} + T_x(\zeta_3)\right)\right].
\end{align}
%
with Jacobians: $\exp{(\eps m^{t} \cdot S_x(\zeta_2))}$, and $\exp{(\eps \bar m^t \cdot S_x(\zeta_3))}$, respectively. 
%
Finally, we proceed to update $v$ again, using the subset $\zeta_4 \equiv (x^{\prime\prime}, \partial_{x}
U^{\prime\prime}, t)$: 
%
\begin{equation} 
  \vpp = \vp \odot \exp\left(\frac{\varepsilon}{2} S_v(\zeta_4)\right) - \frac{\varepsilon}{2}\left[\partial_{x} U
  \odot \exp(\varepsilon Q_v(\zeta_4)) + T_v(\zeta_4)\right].
    \label{eq:update_momentum_forward2}
\end{equation}
%
In order to build some intuition about each of these terms, we discuss below some of the subtleties contained in this
approach and how they are (carefully) dealt with.

The first thing to notice about these equations is that if $S_{i} = Q_{i} = T_{i} = 0$ ($i = x, v$), we recover the
previous equations for the generic leapfrog integrator (as we would expect since we are attempting to
\emph{generalize} HMC).
%
We can also see a similarity between the equations for updating $v$ and those for updating $x$: each update is
generalized by \emph{scaling} the previous value ($v$ or $x$), and \emph{scaling and translating} the updating
value (either $\partial_{x}\,U(x)$ or $x$).
%
It can be shown~\cite{2017arXiv171109268L}, that the scaling applied to the momentum in
Eq~\ref{eq:update_momentum_forward1} can enable, among other things, acceleration in low-density zones to facilitate
mixing between modes, and that the scaling term applied to the gradient may allow better conditioning of the energy
landscape (e.g., by learning a diagonal intertia tensor), or partial ignoring of the energy gradient for rapidly
oscillating energies.
%
Second, note that because the determinant of the Jacobian appears in the Metropolis-Hastings (MH) acceptance
probability, we require the Jacobian of each update to be efficiently computable (i.e.\ independent of the variable
actually being updated).
%
For each of the momentum updates, the input is a subset $\zeta = (x, \partial_{x}\,U(x), t)$ of the augmented space and
the associated Jacobian is $\exp{\left(\frac{\eps}{2}\mathbbm{1}\cdot S_{v}(\zeta)\right)}$ which is independent of $v$
as desired.
%
For the position updates however, things are complicated by the fact that the input $\zeta$ is $x$-dependent.
%
In order to ensure that the Jacobian of the $x$ update is efficiently computable, it is necessary to break the update
into two parts following the approach outlined in \emph{Real-valued Non-Volume Preserving transformations
(RealNVP)}~\cite{dinhRealNVP}.

%
% \subsection{MCMC Transitions}
\subsection{Metropolis-Hastings accept/reject}
%
Written in terms of these transformations, the augmented leapfrog operator $\mathbf{L}_{\theta}$ consists of $M$
sequential applications of the single-step leapfrog operator $\mathbf{L}_{\theta} \xi = \mathbf{L}_{\theta}(x, v, d) =
(x^{\prime\prime\times M}, v^{\prime\prime\times M}, d)$, followed by the previously-defined momentum flip operator
$\mathbf{F}$ which flips the direction variable $d$, i.e.\ $\mathbf{F}\xi = (x, v, -d)$.
%
Using these, we can express a complete molecular dynamics update step as $\mathbf{FL}_{\theta}\xi = \xip$, where now
the Metropolis-Hastings acceptance probability for this proposal is given by
%
\begin{equation}
    A(\mathbf{F}\mathbf{L} \xi | \xi) = \min\left(1,
        \frac{p(\mathbf{F}\mathbf{L}\xi)}{p(\xi)}\left|
        \frac{\partial\left[\mathbf{F}\mathbf{L}\xi\right]}
            {\partial\xi^{T}}\right|\right),
\end{equation}
%
Where $\left|\frac{\partial\left[\mathbf{F}\mathbf{L}\xi\right]} {\partial\xi^{T}}\right|$ denotes the determinant of
the Jacobian describing the transformation.

In contrast to generic HMC where $\left|\frac{\partial\left[\mathbf{F}\mathbf{L}\xi\right]} {\partial\xi^{T}}\right| =
1$, we now have non-symplectic transformations (i.e.\ non-volume preserving) and so we must explicitly account for the
determinant of the Jacobian.
%
These non-volume preserving transformations have the effect of deforming the energy landscape, which, depending on the
nature of the transformation, may allow for the exploration of regions of space which were previously inaccessible.
%
\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.48\textwidth]{energy_barrier}
    \includegraphics[width=0.48\textwidth]{energy_barrier_new}
    \caption{Example of how the determinant of the Jacobian can deform the
      energy landscape. (left) Molecular dynamics (MD) update gets stuck in
      one of the two local minima. (right) After deforming the energy
      landscape, the MD update is able to explore both minima, resulting in a
      better description of the target 
      distribution.}\label{fig:energy_barrier}
\end{figure}
%
To simplify our notation, introduce an additional operator $\mathbf{R}$ that re-samples the momentum and direction,
e.g.\ given $\xi = (x, v, d)$, $\mathbf{R}\,\xi = (x, v^{\prime}, d^{\prime})$ where $v^{\prime} \sim \mathcal{N}(0,
I)$, $d^{\prime} \sim \mathcal{U}\left(\{-1, 1\}\right)$.
%
A complete sampling step of our algorithm then consists of the following two steps:
%
\begin{enumerate}
    \item $\xi^{\prime} = \mathbf{FL}_{\theta} \,\xi$ with probability
        $A(\mathbf{FL}_{\theta}\,\xi|\xi)$ (Eq.~\ref{eq:metropolis_hastings}),
        otherwise $\xi^{\prime} = \xi$.
    \item $\xi^{\prime} = \mathbf{R}\,\xi$.
\end{enumerate}
%
Note however, that for MH to be well-defined, this deterministic operator must be \emph{invertible} and \emph{have a
tractable Jacobian} (i.e.\ we can compute its determinant).
%
In order to make this operator invertible, we augment the state space $(x, v)$ into $(x, v, d)$, where $d \in \{-1,
1\}$ is drawn with equal probability and represent the direction of the update.
%
All of the previous expressions for the augmented leapfrog updates represent the forward ($d = 1$) direction.
%
We can derive the expressions for the backward direction ($d = -1$) by reversing the order of the updates (i.e.\ $\vpp
\rightarrow \vp$, then $\xpp \rightarrow \xp$, followed by $\xp \rightarrow x$ and finally $\vp \rightarrow v$).
%
For completeness, we include in Sec.~\ref{subsubsec:augmented_leapfrog_equations_forward} and
Sec~\ref{subsubsec:augmented_leapfrog_equations_backward} all of the equations (both forward and backward directions)
relevant for updating the variables of interest in our augmented leapfrog sampler.
%
\clearpage
% \subsection{Augmented Leapfrog Equations}%
% \label{subsec:augmented_leapfrog_equations}
%
\subsubsection{Forward direction \texorpdfstring{$(d = 1)$}{(d = 1)}:}%
\label{subsubsec:augmented_leapfrog_equations_forward}
%
\begin{enumerate}
  \item Momentum update: $\,\,\zeta_{1} = (x, \partial_{x}\, U(x), t)$,
    $\,\, \mathcal{J} = \exp{\left(\frac{\eps}{2}\mathbbm{1}\odot S_{v}(\zeta_1)\right)}$
    %
    \vspace{-10pt}
    \begin{equation}
      \vp = v \odot \exp{\left(\frac{\eps}{2}S_{v}(\zeta_{1})\right)}% 
        - \frac{\eps}{2}\left[\partial_{x}\,U(x)\odot \exp{\left(\eps Q_{v}(\zeta_{1})\right)}%
        + T_{v}(\zeta_{1})\right]
    \end{equation}
    \vspace{-40pt}
  \item Position update: $\,\,\zeta_{2} = (x_{\bar{m}^{t}}, v, t)$,
    $\,\, \mathcal{J} = \exp{\left(\eps m^{t}\odot S_{x}(\zeta_2)\right)}$
    %
    \vspace{-10pt}
    \begin{equation}
      \xp = x_{\bar{m}^{t}} + m^{t}\odot \left[x \odot \exp{\left(\eps S_{x}(\zeta_{2})\right)}%
        + \eps\left(\vp\odot\exp{\left(\eps Q_{x}(\zeta_{2})\right)} + T_{x}(\zeta_{2})\right)\right]
    \end{equation}
    \vspace{-40pt}
  \item Position update: $\,\,\zeta_{3} = (x^{\prime}_{m^{t}}, v, t)$,
    $\,\, \mathcal{J} = \exp{\left(\eps \bar{m}^{t}\odot S_{x}(\zeta_3)\right)}$
    %
    \vspace{-10pt}
    \begin{equation}
      \xpp = x^{\prime}_{m^{t}} + \bar{m}^{t}\odot \left[\xp \odot \exp{\left(\eps S_{x}(\zeta_{3})\right)}%
        + \eps\left(\vp\odot\exp{\left(\eps Q_{x}(\zeta_{3})\right)} + T_{x}(\zeta_{3})\right)\right]
    \end{equation}
    \vspace{-40pt}
  \item Momentum update: $\,\,\zeta_{4} = (\xpp, \partial_{x}\, U(\xpp), t)$,
    $\,\, \mathcal{J} = \exp{\left(\frac{\eps}{2}\mathbbm{1}\odot S_{v}(\zeta_{4})\right)}$
    %
    \vspace{-10pt}
    \begin{equation}
      \vpp = \vp \odot \exp{\left(\frac{\eps}{2}S_{v}(\zeta_{4})\right)}%
        - \frac{\eps}{2}\left[\partial_{x}\,U(\xpp)\odot \exp{\left(\eps Q_{v}(\zeta_{4})\right)}%
          + T_{v}(\zeta_{4})\right]
    \end{equation}
    % \vspace{-10pt}
\end{enumerate}
\vspace{-30pt}
%
\subsubsection{Backward direction \texorpdfstring{$(d = -1)$}{(d = -1)}:}%
\label{subsubsec:augmented_leapfrog_equations_backward}
%
\begin{enumerate}
  \item Momentum update: $\,\,\zeta_{1} = (x, \partial_{x} U(x), t)$,
    $\,\, \mathcal{J} = \exp{\left(-\frac{\eps}{2}\mathbbm{1}\odot S_{v}(\zeta_1)\right)}$
    %
    \vspace{-10pt}
    \begin{equation}
      v^{\prime} = {\left\{v - \frac{\eps}{2}\left[\partial_{x}\,U(x)\odot \exp{\left(\eps Q_{v}(\zeta_{1})\right)}%
            + T_{v}(\zeta_{1})\right]\right\}} \odot \exp{\left(-\frac{\eps}{2}S_{v}(\zeta_{1})\right)}
    \end{equation}
    \vspace{-40pt}
  \item Position update: $\,\,\zeta_{2} = (x_{m^{t}}, v, t)$,
    $\,\, \mathcal{J} = \exp{\left(-\eps m^{t}\odot S_{x}(\zeta_2)\right)}$
    %
    \vspace{-10pt}
    \begin{equation}
      \xp = x_{m^{t}} + \bar{m}^{t}\odot%
        {\left[x - \eps{\left(\exp{\left(\eps Q_{x}(\zeta_{2})\right)}\odot \vp%
                + T_{x}(\zeta_{2})\right)}\right]}\odot \exp{\left(-\eps S_{x}(\zeta_{2})\right)}
    \end{equation}
    \vspace{-40pt}
  \item Position update: $\,\,\zeta_{3} = (x^{\prime}_{m^{t}}, v, t)$,
    $\,\, \mathcal{J} = \exp{\left(-\eps \bar{m}^{t}\odot S_{x}(\zeta_3)\right)}$
    %
    \vspace{-10pt}
    \begin{equation}
      \xpp = x_{\bar{m}^{t}} + m^{t}\odot%
        {\left[\xp - \eps{\left(\exp{\left(\eps Q_{x}(\zeta_{3})\right)}\odot \vp%
                + T_{x}(\zeta_{3})\right)}\right]}\odot \exp{\left(-\eps S_{x}(\zeta_{3})\right)}
    \end{equation}
    \vspace{-40pt}
  \item Momentum update: $\,\,\zeta_{4} = (\xpp, \partial_{x} U(\xpp), t)$,
    $\,\, \mathcal{J} = \exp{\left(-\frac{\eps}{2}\mathbbm{1}\odot S_{v}(\zeta_4)\right)}$
    %
    \vspace{-10pt}
    \begin{equation}
      v^{\prime\prime} = {\left\{\vp - \frac{\eps}{2}\left[\partial_{x}\,U(\xpp)\odot%
            \exp{\left(\eps Q_{v}(\zeta_{1})\right)}
            + T_{v}(\zeta_{1})\right]\right\}}\odot 
            \exp{\left(-\frac{\eps}{2}S_{v}(\zeta_{4})\right)}
    \end{equation}
    % \vspace{-10pt}
\end{enumerate}
%
\subsection{Network Architecture}
As previously mentioned, each of the functions $Q$, $S$, and $T$, are implemented using multi-layer perceptrons with
shared weights.
%
It's important to note that we keep separate the network responsible for paramterizing the functions used in the
position updates (`$\Xnet$', i.e.\ $Q_x$, $S_x$, and $T_x$), and the network responsible for parameterizing the
momentum updates (`$\Vnet$', i.e.\ $Q_v$, $S_v$, and $T_v$).
%
Since both networks are identical, we describe the architecture of $\Vnet$ below, and include a flowchart for $\Xnet$ 
illustrative purposes in Fig~\ref{fig:x_net_flowchart}.
%
%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{generic_net/generic_net.pdf}
  \caption{Illustration showing the generic (fully-connected) network architecture for training $S_v$, $Q_v$, and
  $T_v$.}%
\label{fig:generic_net}
\end{figure}
%
%
% Each of these networks have identical architectures, and without loss of generality we describe the architecture of the
% position network below.
%
% Since the networks responsible for updating the, we describe the network architecture responsible for updating the
% momentum $v$.
The network takes as input $\zeta_1 = (x, \partial_{x} U(x), t)$, where $x, v \in \mathbb{R}^{n}$, and $t$ is encoded
as $\tau(t) = \left(\cos{(\frac{2\pi t}{M})},\right.  \left.\sin{(\frac{2\pi t}{M})}\right)$.
%
Each of the inputs is then passed through a fully-connected (`dense' layer), consisting of $n_h$ hidden units
%
\begin{align}
    \tilde x &= W^{(x)} x + b^{(x)} \quad (\in \mathbb{R}^{n_h})\\
    \tilde v &= W^{(v)} v + b^{(v)} \quad (\in \mathbb{R}^{n_h})\\
    \tilde \tau &= W^{(\tau)} \tau + b^{(\tau)} \quad (\in \mathbb{R}^{n_h}).
\end{align}
%
Where $W^{(x)}, W^{(v)} \in \mathbb{R}^{n \times n_h}$, $W^{(t)} \in \mathbb{R}^{2 \times n_h}$, and $b^{(x)}$,
$b^{(v)}$,  $b^{(t)} \in \mathbb{R}^{n_h}$.
%
From these, the network computes
%
\begin{equation}
    h_1 = \sigma(\tilde x + \tilde v + \tilde \tau) \quad (\in
    \mathbb{R}^{n_h}).
    \label{eq:hidden_1}
\end{equation}
%
Where $\sigma(x) = \max(0, x)$ denotes the rectified linear unit (ReLU) activation function.
%
Next, the network computes
%
\begin{equation}
    h_2 = \sigma\left(W^{(h_1)} h_1 + b^{(h_1)}\right) \quad (\in
    \mathbb{R}^{n_h}).
    \label{eq:hidden_2}
\end{equation}
%
These weights ($h_2$) are then used to compute the network's output:
%
\begin{align}
    S_x &= \lambda_s \tanh(W^{(s)} h_2 + b^{(s)})\quad (\in \mathbb{R}^{n})\\
    Q_x &= \lambda_q \tanh(W^{(q)} h_2 + b^{(q)})\quad (\in \mathbb{R}^{n})\\
    T_x &= W^{(T)} h_2 + b^{(T)}\quad (\in \mathbb{R}^{n}),
\end{align}
%
Where $W^{(s)}, W^{(q)}$, and $W^{(T)} \in \mathbb{R}^{n_h \times n}$ and $b^{(s)}, b^{(q)}$, and $b^{(T)} \in
\mathbb{R}^{n}$.
%
The parameters $\lambda_s$ and $\lambda_q$ are additional trainable variables initialized to zero.
%
The network used for parameterizing the functions $T_v$, $Q_v$ and $S_v$ takes as input $(x, \partial_x U(x), t)$ where
again $t$ is encoded as above.  The architecture of this network is the same, and produces outputs $T_v$, $Q_v$, and
$S_v$.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{generic_net/x_net_flowchart4.png}
  \caption{Flowchart illustrating the generic fully-connected network architecture including the intermediate variables
  computed at each hidden layer of the network.}%
\label{fig:x_net_flowchart}
\end{figure}

%
\subsection{Training Procedure}
%
By augmenting traditional HMC methods with these trainable functions, we hope to obtain a sampler that has the
following key properties:
%
\begin{enumerate}
    \item Fast mixing (i.e.\ able to quickly produce uncorrelated samples).
    \item Fast burn-in (i.e.\ rapid convergence to the target distribution).
    \item Ability to mix across energy levels.
    \item Ability to mix between modes.
\end{enumerate}
%
Following the results in~\cite{10.2307/24308995}, we design a loss function with the goal of maximizing the eqpected
squared jumped distance (or analogously, minimizng the lag-one autocorrelation).
%
To do this, we first introduce 
\begin{equation}
  \delta(\xi, \xip) = \delta((x^{\prime}, v^{\prime}, d^{\prime}), (x, v, d)) \equiv \| x - x^{\prime}\|^2_2.
  \label{eq:metric_orig}
\end{equation}
%
Then, the expected squared jumped distance is given by $\mathbb{E}_{\xi\sim p(\xi)}
\left[\delta(\mathbf{FL}_{\theta}\xi, \xi) A(\mathbf{FL}_{\theta}\xi | \xi)\right]$.
%
By maximizing this objective function, we are encouraging transitions that efficiently explore a local region of
state-space, but may fail to explore regions where very little mixing occurs.
%
To help combat this effect, we define a loss function
%
\begin{equation}
    \ell_{\lambda}(\xi, \xi^{\prime}, A(\xi^{\prime}|\xi)) =
        \frac{\lambda^2}{\delta(\xi,\xi^{\prime}) A(\xi^{\prime}|\xi)} -
        \frac{\delta(\xi,\xi^{\prime}) A(\xi^{\prime}|\xi)}{\lambda^2}
    \label{eq:loss_ell}
\end{equation}
%
where $\lambda$ is a scale parameter describing the characteristic length scale of the problem.
%
Note that the first term helps to prevent the sampler from becoming stuck in a state where it cannot move effectively,
and the second term helps to maximize the distance between subsequent moves in the Markov chain.  The sampler is then
trained by minimizing $\ell_{\lambda}$ over both the target and initialization distributions.
%
Explicitly, for an initial distribution $\pi_0$ over $\mathcal{X}$, we define the initialization distribution as
$q(\xi) = \pi_0(x) \mathcal{N}(v; 0, I) p(d)$, and minimize
%
\begin{equation}
    \mathcal{L}(\theta)\equiv \mathbb{E}_{p(\xi)}\left[\ell_{\lambda}(\xi,
    \mathbf{FL}_{\theta}\xi, A(\mathbf{FL}_{\theta}\xi|\xi))\right] + \lambda_b
    \mathbb{E}_{q(\xi)}\left[\ell_{\lambda}(\xi, \mathbf{FL}_{\theta}\xi,
    A(\mathbf{FL}_{\theta} \xi| \xi))\right].
    \label{eq:loss_L}
\end{equation}
%
For completeness, we include the full algorithm~\cite{2017arXiv171109268L} used to train L2HMC in Alg.~\ref{alg:l2hmc}.
% \Input{\textbf{(1.)} Energy function $U: \mathcal{X} \rightarrow \mathbb{R}$ and its
%   gradient $\nabla_x U: \mathcal{X} \rightarrow \mathcal{X}$, \textbf{(2.)} initial
%   distribution over the augmented state space $q$, \textbf{(3.)} number of
%   iterations $n_{\text{iters}}$, \textbf{(4.)} number of leapfrog steps $M$,
%   \textbf{(5.)} learning rate schedule ${(\alpha_{t})}_{t\leq n_{\text{iters}}}$,
%   \textbf{(6.)} batch size $N$, \textbf{(7.)} scale parameter $\lambda$,
%   and \textbf{(8.)} regularization strength $\lambda_b$.}\;

\begin{algorithm}[H]
    \SetKwProg{Fn}{def}{\string:}{}%
    \SetKwFunction{Range}{range}%
    \SetKwFor{For}{for}{\string:}{}%
    \SetKwIF{If}{ElseIf}{Else}{if}{:}{elif}{else:}{}%
    \SetKwFor{While}{while}{:}{fintq}%
    \AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%
    \DontPrintSemicolon%
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}%
    \caption{Training procedure for L2HMC.}%
    \Input{%
      \vspace{-5pt}
      \begin{enumerate}
        \item Energy function $U: \mathcal{X} \rightarrow \mathbb{R}$ and its gradient $\nabla_x U: \mathcal{X}
          \rightarrow \mathcal{X}$\
          \vspace{-15px}
        \item Initial distribution over the augmented state space $q$
          \vspace{-15px}
        \item Number of iterations $N_{\mathrm{train}}$
          \vspace{-15px}
        \item Number of leapfrog steps $N_{\mathrm{LF}}$
          \vspace{-15px}
        \item Learning rate schedule ${(\alpha_{t})}_{t\leq N_{\text{train}}}$
          \vspace{-15px}
        \item Batch size $N_{\mathrm{samples}}$
          \vspace{-15px}
        \item Scale parameter $\lambda$
          \vspace{-15px}
        \item Regularization strength $\lambda_b$.
      \end{enumerate}
    }\;
    \vspace{-20px}
    Initialize the parameters of the sampler $\theta$.\;
    Initialize ${\{\xi_{p^{(i)}}\}}_{i\leq N_{\mathrm{samples}}}$ from $q{(\xi)}$.\;
    \For{$t = 0$ \KwTo\ $N_{\mathrm{train}}$}{%
      Sample a minibatch ${\left\{\xi_{q}^{(i)}\right\}}_{i\leq N_{\mathrm{samples}}}$ from $q{(\xi)}$.\;
      $\mathcal{L}$ $\leftarrow\ 0$\;
      \For{$i = 1$ \KwTo$N_{\mathrm{LF}}$} {%
        $\xi_{p}^{(i)} \leftarrow\ \mathbf{R}\,\xi_p^{(i)}$\;
        % \tcp*{\tiny{resample the momentum}}\;
        % \tcp{\tiny{perform MD using augmented leapfrog, calc.\ loss}}\;
        $\mathcal{L} \leftarrow \mathcal{L} + \ell_{\lambda}\left(\xi_p^{(i)}, \FLq\xi_p^{(i)},
          A(\FLq\xi^{(i)}_p|\xi^{(i)}_p)\right) + \lambda_b \ell_{\lambda}\left(\xi^{(i)}_q, \FLq\xi^{(i)}_q,
          A(\FLq\xi^{(i)}_q|\xi^{(i)}_q)\right)$\;

        $\xi_p^{(i)} \leftarrow \FLq\xi^{(i)}_p$ with probability $A(\FLq\xi^{(i)}_p|\xi^{(i)}_p)$\; 
      }
      $\theta\ \leftarrow\ \theta-\alpha_t \nabla_{\theta} \mathcal{L}$\;
    }%
\label{alg:l2hmc}
\end{algorithm}
    

\section{Gaussian Mixture Model}%
\label{sec:l2hmc_gmm}
%
The Gaussian Mixture Model (GMM) is a notoriously difficult example for traditional HMC to sample accurately due to the
existence of multiple modes.
%
In particular, HMC cannot mix between modes that are reasonably separated without recourse to additional tricks.
%
This is due, in part, to the fact that HMC cannot easily traverse the low-density zones which exist between modes.

In the most general case, we consider a target distribution described by a mixture of $M > 1$ components in
$\mathbb{R}^{D}$ for $D \geq 1$:
%
\begin{equation}
    p(\mathbf{x}) \equiv \sum_{m=1}^{M} p(m) p(\mathbf{x}|m) \equiv
        \sum_{m=1}^{M} \pi_m p(\mathbf{x}|m) \quad \forall \,\,\mathbf{x} \in
        \mathbb{R}^{D}
    \label{eq:gmm_model}
\end{equation}
%
where $\sum_{m=1}^{M} \pi_m = 1$, $\pi_M \in (0, 1)$ $\forall m = 1, \ldots, M$ and each component distribution is a
normal probability distribution in $\mathbb{R}^{D}$.
%
So $\mathbf{x}|m \sim \mathcal{N}(\bm{\mu}_m, \bm{\Sigma}_m)$, where $\bm{\mu}_m \equiv
\mathbb{E}_{p{(\mathbf{x}|m)}}\left\{\mathbf{x}\right\}$ and $\mathbf{\Sigma}_m \equiv
\mathbb{E}_{p{(\mathbf{x}|m)}}{\left\{{(\mathbf{x} - \bm{\mu}_m)}{(\mathbf{x} - \bm{\mu}_m)}^{T}\right\}} > 0$ are the
mean vector and covariance matrix, respectively, of component $m$.
%
\subsection{Example}
%
Consider a simple 2D case consisting of two Gaussians 
%
\begin{equation}
    \mathbf{x} \sim \pi_1 \,\mathcal{N}(\bm{\mu}_1, \bm{\Sigma}_1) +
        \pi_2\, \mathcal{N}(\bm{\mu}_2, \bm{\Sigma}_2)
    \label{eq:log_likelihood_example}
\end{equation}
%
with $\pi_1 = \pi_2 = 0.5$, $\bm{\mu}_1 = (-2, 0)$, $\bm{\mu}_2 = (2, 0)$ and
%
\begin{equation}
    \bm{\Sigma}_1 = \bm{\Sigma}_2 = 
        \begin{bmatrix}
            0.1    & 0 \\
            0       & 0.1 
        \end{bmatrix}
    \label{eq:covariance_matrix}
\end{equation}
%
The results of trajectories generated using both traditional HMC and the L2HMC algorithm can be seen in
Fig.~\ref{fig:trajectories}.
%
Note that traditional HMC performs poorly and is unable to mix between the two modes, whereas L2HMC is able to
correctly sample from the target distribution without getting stuck in either of the individual modes.

%
The L2HMC sampler was trained using simulated annealing using the schedule shown in Eq~\ref{eq:gmm_annealing} with a
starting temperature of $T = 10$, for $5,000$ training steps.
%
By starting with a high temperature, the chain is able to move between both modes (`tunnel') successfully.
%
Once it has learned this, we can lower the temperature back to $T = 1$ and recovering the initial distribution while
preserving information about tunneling in the networks ``memory''.
%
\begin{equation}
  T(n) = {\left(T_{i} - T_{f}\right)} \cdot {\left(1 - \frac{n}{N_{\mathrm{train}}}\right)} + T_{f}
  \label{eq:gmm_annealing}
\end{equation}
%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.98\textwidth]{gmm_figures/iso_gmm_chains1}
    \caption{Comparison of trajectories generated using L2HMC (top), and
        traditional HMC with $\eps = 0.25$ (middle) and $\eps = 0.5$ (bottom).
        Note that L2HMC is able to successfully mix between modes, whereas HMC
        is not.}\label{fig:trajectories}
\end{figure}
%
\section{2D \texorpdfstring{$U(1)$}{U(1)} Lattice Gauge Theory}  %chktex 36
\label{sec:l2hmc_u1}
All lattice QCD simulations are performed at finite lattice spacing $a$ and need an extrapolation to the continuum in
order to be used for computing values of physical quantities.
%
More reliable extrapolations can be done by simulating the theory at increasingly smaller lattice spacings.
%
The picture that results when the lattice spacing is reduced and the physics kept constant is that all finite physical
quantities of negative mass dimension diverge if measured in lattice units.
%
In statistical mechanics language, this states that the continuum limit is a critical point of the theory since
correlation lengths diverge.
%
MCMC algorithms are known to encounter difficulties when used for simulating theories close to a critical point, an
issue kown as the \emph{critical slowing down} of the algorithm.
% Currently, simulations in lattice QCD are plagued with the issue of \emph{critical slowing down} as we move towards the
% continuum limit.
%
This effect is most prominent in the topological charge, whose auto-correlation time increases dramatically with finer
lattice spacings.
%
As a result, there is a growing interest in developing new sampling techniques for generating equilibrium
configurations. 
%
In particular, algorithms that are able to offer improvements in efficiency through a reduction of statistical
autocorrelations are highly desired. 
%

% Building off the results from the two-dimensional Gaussian mixture model, we are interested to see how this algorithm
% performs for simulations in gauge theory and lattice QCD.
%

We begin with the two-dimensional $U{(1)}$ lattice gauge theory with dynamical variables $U_{\mu}{(i)}$
defined on the links of a lattice, where $i$ labels a site and $\mu$ specifies the direction.
%
%
Each link $U_{\mu}{(i)}$ can be expressed in terms of an angle $-\pi < \phi_{\mu}{(i)} \leq \pi$.
%
\begin{equation}
    U_{\mu}{(i)} = e^{i\phi_{\mu}{(i)}}
    \label{eq:link_variable}
\end{equation}
%
with the Wilson action defined as:
%
\begin{equation}
    \beta S = \beta \sum_{P}{(1 - \cos{(\phi_{P})})}
    \label{eq:wilson_action}
\end{equation}
%
where
%
\begin{equation}
    \phi_{P} \equiv \phi_{\mu\nu}(i) = 
        \phi_{\mu}{(i)} + \phi_{\nu}{(i + \hat{\mu})} 
        - \phi_{\mu}{(i + \hat{\nu})} - \phi_{\nu}{(i)}
    \label{eq:phi_plaquette}
\end{equation}
%theta_
and $\beta = 1/e^{2}$ is the gauge coupling, and the sum $\sum_{P}$ runs over all plaquettes of the lattice.
%
An illustration showing how these variables are defined for an elementary plaquette is shown in
Fig.~\ref{fig:plaquette}.
%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.5\textwidth]{gauge_figures/plaquette.pdf}
  \caption{Illustration of an elementary plaquette on the lattice.}%
\label{fig:plaquette}
\end{figure}

We can define the topological charge, $Q \in \mathbb{Z}$, as
%
\begin{equation}
  Q \equiv \frac{1}{2\pi}\sum_{P} \tilde \phi_{P} =
    \frac{1}{2\pi}\sum_{\substack{{i; \mu, \nu}\\{\nu > \mu}}}
    \tilde \phi_{\mu\nu}{(i)}
    \label{eq:topological_charge}
\end{equation}
%
where
%
\begin{equation}
  \tilde{\phi}_{P} \equiv \phi_{P} - 2\pi {\bigg\lfloor{\frac{\phi_{P} + \pi}{2\pi}\bigg\rfloor}}
  % \tilde{\phi}_{P} \equiv \phi_{P} - 2 \pi \left \lfloor{\frac{\phi_{P} + \pi}{2 \pi}\right \rfloor}
\end{equation}
%
is the sum of the link variables around the elementary plaquette, projected onto the interval $\left[-\pi, \pi\right)$.
From this, we can define topological susceptibility
%
\begin{equation}
    \chi \equiv \frac{\langle Q^2\rangle - \langle Q \rangle^2}{V}
    % \label{eq:topological_susceptibility}
\end{equation}
%
By parity symmetry, $\langle Q \rangle = 0$, so we have that
\begin{equation}
    \chi = \frac{\langle Q^2\rangle}{V}
    \label{eq:topological_susceptibility}
\end{equation}
%
Unfortunately, the measurement of $\chi$ is often difficult due to the fact that the autocorrelation time with respect
to $Q$ tends to be extremely long.
%
This is a consequence of the fact that the Markov chain tends to get stuck in a topological sector (characterized by $Q
= const$.), a phenomenon known as \emph{topological freezing}.
%
\begin{figure}[htpb]
  \centering
    % \includegraphics[width=0.49\textwidth]{top_charge_vs_step_hmc.eps}
    % \includegraphics[width=0.49\textwidth]{top_charge_vs_step_l2hmc.eps}
    \includegraphics[width=0.49\textwidth]{charge_plots/compare/top_charge_vs_step_hmc}
    \includegraphics[width=0.49\textwidth]{charge_plots/compare/top_charge_vs_step_l2hmc}
    \caption{(left) Example of topological freezing in the $2D$ $U{(1)}$
      lattice gauge theory, generated from generic HMC sampling for a
      $16\times16$ lattice. Note that for the majority of the simulation
      $Q=-2$, making it virtually impossible to get a reasonable estimate of
      $\chi$.  (right) Topological charge vs.\ step generated using the trained
      L2HMC sampler.}\label{fig:top_charge}
\end{figure}
%
%
% \begin{figure}[htpb]\label{fig:top_charge_l2hmc}
%     \centering
%     \includegraphics[width=0.66\textwidth]{top_charge_vs_step_l2hmc1}
%     \caption{Topological charge vs. step for a chain generated using trained
%       L2HMC sampler on a $16\times16$ lattice.}
%   \end{figure}
%
%
\subsection{Modified Network Architecture}
\label{subsec:l2hmc_modified_network}
%
In order to better account for the rectangular geometry of the lattice, a stack of convolutional layers was prepended
to the existing architecture, and can be seen in Fig.~\ref{fig:conv_net}. The output from this convolutional structure
is then fed to the generic network shown in Fig.~\ref{fig:generic_net}.
%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{conv_net/conv_net.pdf}
  % \includegraphics[width=\textwidth]{full_network/generic_net.png}
  \caption{Convolutional structure used for learning localized features of
    rectangular lattice.}%
\label{fig:conv_net}
\end{figure}
% \vspace{-40pt}
%
\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.95\textwidth]{conv_net/vnet_hq.png}
  \caption{Illustration taken from TensorBoard showing an overview of the network architecture for VNet. Note that the
    architecture is identical for XNet.}
    \vspace{12pt}
  \includegraphics[width=0.95\textwidth]{conv_net/vnet_zoom_hq.png}
  \caption{Detailed view of additional convolutional structure included to better account for rectangular geometry of
  lattice inputs.}
\end{figure}
%
Additionally, the network architecture was modified to include a batch normalization layer after the second MaxPool
layer.
%
Introducing batch normalization is a commonly used technique in practice, and is known to help prevent against
diverging gradients\footnote{a numerical issue in which infinite values are generated when calculating the gradients in
backpropagation}, (an issue that was occasionally encountered during the training procedure).
%
Additionally, it has been shown to improve model performance and generally requires fewer training steps to achieve
similar performance as models trained without it~\cite{batch_norm}.
%
\subsection{Annealing Schedule}%
\label{subsec:l2hmc_u1annealing}
% In addition to modifying the neural network architecture, we also modified the training algorithm to follow a simulated
% annealing schedule.
%
Proceeding as in the example of the Gaussian Mixture Model, we include a simulated annealing
schedule in which the value of the gauge coupling $\beta$ is continuously updated according to the annealing schedule
shown in Eq.~\ref{eq:annealing_schedule}.
%
% Explicitly, the value of the gauge coupling $\beta$ is continuously updated according to the annealing schedule shown
% in Eq.~\ref{eq:annealing_schedule}.
%
This was done in order to encourage sampling from multiple different topological charge sectors, since our sampler is
less `restricted' at lower values of $\beta$.
%This was don
\begin{equation}
  \frac{1}{\beta(n)} = {\left(\frac{1}{\beta_{i}} - \frac{1}{\beta_{f}}\right)} {\left(\frac{1 -
  n}{N_{\mathrm{train}}}\right)} + \frac{1}{\beta_{f}}
    \label{eq:annealing_schedule}
\end{equation}
%
Here $\beta(n)$ denotes the value of $\beta$ to be used for the $n^{\mathrm{th}}$ training step ($n = 1, \ldots,
N_{\mathrm{train}}$), $\beta_{i}$ represents the initial value of $\beta$ at the beginning of the training, and
$\beta_{f}$ represents the final value of $\beta$ at the end of training.
%
For a typical training session, $N_{\mathrm{train}} = 25,000$, $\beta_{i} = 2$ and $\beta_{f} = 5$.
% For all of the exmaples above, $N_{\mathrm{train}} = 25,000$, $\beta_{0} = 2$
% and $\beta_{N_{\mathrm{train}}} = 5$.
%
% As can be seen in Fig.~\ref{fig:top_charge}

% \section{UPDATES (04/08/2019)}
%
\subsection{Modified loss function for \texorpdfstring{$U(1)$}{U (1)} gauge model}
\label{subsec:l2hmc_modifiedloss}
%
In order to more accurately define the ``distance'' between two different lattice configurations, we redefine the
metric in Eq.~\ref{eq:metric_orig} to be
%
\begin{equation}
  \delta(\xi(\phi_{\mu}(x)), \xip(\phi_{\mu}(x))) \equiv 1 - \cos\left(\xi(\phi_{\mu}(x)) - \xi^{\prime}(\phi_{\mu}(x))\right)
  \label{eq:metric_new}
\end{equation}
%
where $x$ runs over all lattice sites\footnote{In what follows, we will refrain from explicitly including the site
index and make the assumption that it implicitly extends over all sites on the lattice.} and  $\mu=0, 1$ for the two
dimensional case.
%
Note that this metric gives the expected behavior, since $\delta \rightarrow 0$ for $\xi \approx \xip$.

While this new metric helps to better measure distances in this configuration space, it does nothing to encourage the
exploration of different topological sectors since there may be configurations for which $\delta(\xi, \xip) \approx 1$
but $Q(\xi) = Q(\xip)$.
%
In order to potentially address this issue, we modify the original loss function as follows.


First define $\xip\equiv \FLq \xi$ as the resultant configuration proposed by the augmented leapfrog integrator, and
%
\begin{align}
  \delta_{Q}{(\xi, \xi^{\prime})} &= \left|Q{(\xi)} - Q{(\xi^{\prime})}\right| \\
  \ell_{Q}{\left(\xi, \xip, A{(\FLq\xi|\xi)}\right)} &= \delta_{Q}{(\xi,\xip)}
    \times A{(\xip|\xi)}.
\end{align}
%
So we have that $\delta_{Q}$ measures the difference in topological charge between the initial and proposed
configurations, and $\ell_{Q}$ gives the expected topological charge difference.
%
Proceeding as before, we include an additional auxiliary term which is identical in structure to the one above, except
the input is now a configuration of link variables $\phi_{\mu}$ drawn from the initialization distribution $q$, which
for our purposes was chosen to be the standard random normal distribution on $[0, 2\pi)$. %]
% (
% $\mathopen[0, 2\pi\mathopen)$.
% ]

We can then write the topological loss term as
%
\begin{equation}
  \mathcal{L}_{Q}(\theta) \equiv
    \mathbb{E}_{p(\xi)}{\left[\ell_{Q}{\left(\xi,
      \FLq \xi, A{(\FLq\xi|\xi)}\right)}\right]}
      + \alpha_{\mathrm{aux}}\, \mathbb{E}_{q(\xi)}{\left[\ell_{Q}{\left(\xi,
      \FLq \xi, A{(\FLq\xi|\xi)}\right)}\right]}
      \label{eq:topological_loss_term1}
\end{equation}
%
If we denote the standard loss (with the modified metric function) defined in Eq.~\ref{eq:loss_L} as
$\mathcal{L}_{\mathrm{std}}{(\theta)}$, we can write the new total loss as a combination of these two terms,
%
\begin{equation}
  \mathcal{L}(\theta) =
    \alpha_{\mathrm{std}}\, \mathcal{L}_{\mathrm{std}}(\theta) 
    + \alpha_{Q}\, \mathcal{L}_{Q}(\theta)
    \label{eq:topological_loss_term}
\end{equation}
%
where $\alpha_{\mathrm{std}}, \alpha_Q$ are multiplicative factors that weigh the relative contributions to the total
loss from the standard and topological loss terms respectively, and $\alpha_{\mathrm{aux}}$ in
Eq.~\ref{eq:topological_loss_term1} weighs the contribution of configurations drawn from the initialization
distribution.
%
\subsection*{Issues with the Average Plaquette}
%
Upon further testing, an issue was encountered in which the average plaquette $\langle \phi_{P}\rangle$ seems to
converge to a value which is noticeably different from the expected value in the infinite volume limit.
%
This behavior can be seen in Fig.~\ref{fig:bad_convergence}, and seems to depend on both the number of augmented
leapfrog steps used by our integrator, as well as the `strength' of the topological loss term in
Eq.~\ref{eq:topological_loss_term}.
%
\begin{figure}[htpb]\label{fig:bad_convergence}
  \centering
  \includegraphics[width=0.49\textwidth]{plaq_plots/plaqs_vs_step_bad_converge} 
  \includegraphics[width=0.49\textwidth]{plaq_plots/plaqs_vs_step_good_converge} 
  \caption{Average plaquette $\langle\phi_{P}\rangle$ vs.\ step for $L=8$. Each
    colored line represents a unique chain, and the black line shows the
    average over all chains.  (left) $\alpha_{Q} = 1$, illustrating the failure
    to converge to the expected value (denoted by the solid red line), while
    (right) $\alpha_{Q} = 0$ behaves as expected.}
\end{figure}
%
In order to quantify this unexpected behavior, we denote by $\phi_{P}^{(\mathrm{obs})}$ the \emph{observed} value of
the average plaquette $\phi_{P}$ and $\phi_{P}^{(\mathrm{exp})}$ the \emph{expected} value, we can define the quantity
%
\begin{equation}
  {\delta_{\phi_P}(\alpha_Q, N_{\mathrm{LF}}) \equiv \langle \phi_P^{\mathrm{(obs)}}\rangle 
  - \langle{\phi_{P}^{\mathrm{(exp)}}}\rangle \neq 0}.
\end{equation}
%
Which allows us to measure the severity of this discrepancy.%
%
% After many unsuccessful attempts and determining the cause of this behavior, I'm still not entirely sure what is
% causing it.
%
% The difference between the observed value and the expected value also seems to increase as more leapfrog steps are
% performed per MD update as well.

Also, as expected, when ran for the same number of leapfrog steps and everything else being the same, the difference
between the observed and expected value of $\langle\phi_{P}\rangle$ increases as $\alpha_{Q}: 0 \rightarrow 1$.
%
As Xiao-Yong suggested that it might be due to precision issues or round-off errors being carried through, I tried
re-running everything using \texttt{float64} instead of \texttt{float32} but it didn't seem to make any difference,
$\langle\phi_{P}\rangle$ still failed to converge to the expected value.

As an alternative, I've also tried replicating te $\ell$ term from the original loss function Eq.~\ref{eq:loss_ell}
exactly with $\delta(\xi,\xip) \rightarrow \delta_{Q}(\xi,\xip)$ but this proved to be very unstable and the network
was unable to get more than a few training steps in before diverging.

Additionally, I tried repeating the training/evaluation process using
multiple different values of both $N_{\mathrm{LF}}$ and $\alpha_Q$ ($\alpha_Q$ defined in
Eq.~\ref{eq:topological_loss_term}).
%
Strangely, the issue seems to be almost irreproducible and doesn't seem to follow a clear dependence on either
$N_{\mathrm{LF}}$ or $\alpha_Q$.
%
I've included plots illustrating the difference $\delta_{\phi_{P}}$ vs. MD step\footnote{Note that each molecular
dynamics (MD) step consists of a momentum refreshment followed by $N_{\mathrm{LF}}$ leapfrog steps, and finally a
Metropolis-Hastings accept/reject.} below obtained using both the trained L2HMC sampler as well as the generic HMC
sampler for comparison.
%
In each of the plots below, the L2HMC sampler was trained for $25,000$ steps with a simulated annealing schedule
beginning at $\beta = 2.0$ and ending at $\beta = 5.0$.
%
Additionally, the training was distributed across $16$ nodes on COOLEY using \texttt{horovod}.
%
The results are averaged over all $N_{\mathrm{samples}}$ samples in the
mini-batch.

I'm currently working on re-running all of the following experiments to see if this difference $\plaqdiff$ is
reproducible or if it is some sort of anomaly.

% \subsubsection*{Diverging gradients}
% On a separate `debugging' note, previously I had mentioned that I would occasionally run into an issue where the
% gradients would (seemingly randomly) blow up during training. I had spoken with Xiao-Yong about this in a little more
% depth and I explained that I believed it was directly related to the gradient of the step size, which I was able to
% occasionally avoid by introducing a gradient clipping operation during backpropagation. This approach seemed to work
% well in some cases but would still occasionally diverge.
%
%
% \subsection*{Topological susceptibility \texorpdfstring{$\chi$}{χ:}}
% %
% \begin{figure}[htpb]
%   \centering
%   \includegraphics[width=0.95\textwidth]{suscept_plots/topological_suscept_vs_beta_all_10}
%   \caption{Topological susceptibility vs. $\beta$ for various lattice sizes
%   $L = 8, 12, 16$. Note that the data points in the inset plots are slightly
%   shifted horizontally from each other for illustrative purposes.}%
%   \label{fig:suscept}
% \end{figure}
% %
% \begin{itemize}
%   \item All statistics for $\chi$ were calculated by running either the
%     trained L2HMC sampler or a generic HMC sampler with equivalent
%     parameters (step size at the end of training, number of leapfrog steps,
%     etc.).  %
%   \item For $N_{\mathrm{MD}}$ molecular dynamics (accept/reject) update
%     steps for $N_{\mathrm{chains}}$ (batch size) chains in parallel.  %
%   \item Each molecular dynamics update step consists of $5$ individual
%     leapfrog steps (augmented leapfrog step for L2HMC sampler).  %
%   \item For the results shown in Fig.~\ref{fig:suscept}, $N_{\mathrm{MD}} =
%     1\times10^4$ for all values of $L$, and $N_{\mathrm{chains}} = 32,
%     50, 64$ for $L = 16, 12$, and $8$ respectively.
% \end{itemize}
% %
% \begin{figure}[htpb]%\label{fig:top_charges8_l2hmc}
%   \centering
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step8_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step8_HMC}
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step12_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step12_HMC}
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step16_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step16_HMC}
%   \caption{Topological charge of a single chain vs.\ evaluation step at
%   $\beta=5$. Top row: $L=8$, Middle row: $L=12$, bottom row $L=16$. Left
%   column: trained L2HMC sampler, right column: generic HMC sampler}
% \end{figure}
% %
% \begin{figure}[htpb]%\label{}
%   \centering
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step8_b4_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step8_b4_HMC}
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step12_b4_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step12_b4_HMC}
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step16_b4_L2HMC}
%   \hfill
%   \includegraphics[width=0.49\textwidth]{charge_plots/top_charge_vs_step16_b4_HMC}
%   \caption{Topological charge of a single chain vs.\ evaluation step at
%     $\beta=4$. Top row: $L=8$, Middle row: $L=12$, bottom row $L=16$. Left
%     column: trained L2HMC sampler, right column: generic HMC sampler}
% \end{figure}
%
% \section{UPDATES (04/22/2019) Autocorrelation of topological charge \texorpdfstring{$Q$}{Q}}
% We are interested in the autocorrelation of the topological charge $Q$.
% %
% \begin{figure}[htpb]
%   \centering
%   \includegraphics[width=0.48\textwidth]{autocorrelations/top_charge_autocorrelations_L8_steps3_batch256}
%   \includegraphics[width=0.48\textwidth]{autocorrelations/top_charge_autocorrelations_L8_steps5_batch128}
%   \caption{Autocorrelation of the topological charge $Q$, on an $8 \times 8$
%     lattice, averaged over $128$ samples. The molecular dynamics
%   update consisted of $3$ (left) and $5$ (right) augmented (L2HMC) leapfrog steps.}
%   \label{fig:charge_acorr_L8}
% \end{figure}
% %
% \begin{figure}[htpb]
%   \centering
%   \includegraphics[width=0.48\textwidth]{autocorrelations/top_charge_autocorrelations_L16_steps3_batch256}
%   \includegraphics[width=0.48\textwidth]{autocorrelations/top_charge_autocorrelations_L16_steps5_batch128}
%   \caption{Autocorrelation of the topological charge $Q$, on an $16 \times 16$
%     lattice, averaged over $128$ samples. The molecular dynamics
%   update consisted of $3$ (left) and $5$ (right) leapfrog steps.}
%   \label{fig:charge_acorr_L16}
% \end{figure}
%
% The model was trained using simulated annealing for $N_{\mathrm{train}}$ training steps.
% %
% The paramter $\beta$ was updated according to the annealing schedule shown in Eq.~\ref{eq:annealing_schedule}.
% %
% \begin{equation}
%   \frac{1}{\beta(n)} = {\left(\frac{1}{\beta_{i}} - \frac{1}{\beta_{f}}\right)} {\left(\frac{1 -
%   n}{N_{\mathrm{train}}}\right)} + \frac{1}{\beta_{f}}
%     \label{eq:annealing_schedule}
% \end{equation}
% %
% Here $\beta(n)$ denotes the value of beta to be used for the $n^{\mathrm{th}}$ training step ($n = 1, \ldots,
% N_{\mathrm{train}}$), $\beta_{i}$ represents the initial value of $\beta$ at the beginning of the training, and
% $\beta_{f}$ represents the final value of $\beta$ at the end of training.
% %
% For all of the exmaples above, $N_{\mathrm{train}} = 25,000$, $\beta_{0} = 2$
% and $\beta_{N_{\mathrm{train}}} = 5$.

% \section{UPDATES (05/03/2019)}
% Previously we had noticed that there seems to be a discrepancy between the observed and expected value of the average
% plaquette,
% %
% \begin{equation}
%   {\delta_{\phi_P}(\alpha_Q, N_{\mathrm{LF}}) \equiv \langle \phi_P^{\mathrm{(obs)}}\rangle
%   - \langle{\phi_{P}^{\mathrm{(exp)}}}\rangle \neq 0}.
% \end{equation}
% %
% In an attempt to better understand why this was occurring, I tried repeating the training/evaluation process using
% multiple different values of both $N_{\mathrm{LF}}$ and $\alpha_Q$ ($\alpha_Q$ defined in
% Eq.~\ref{eq:topological_loss_term}).
% %
% Strangely, the issue seems to be almost irreproducible and doesn't seem to follow a clear dependence on either
% $N_{\mathrm{LF}}$ or $\alpha_Q$.
% %
% I've included plots illustrating the difference $\delta_{\phi_{P}}$ vs. MD step\footnote{Note that each molecular
% dynamics (MD) step consists of a momentum refreshment followed by $N_{\mathrm{LF}}$ leapfrog steps, and finally a
% Metropolis-Hastings accept/reject.} below obtained using both the trained L2HMC sampler as well as the generic HMC
% sampler for comparison.
% %
% In each of the plots below, the L2HMC sampler was trained for $25,000$ steps with a simulated annealing schedule
% beginning at $\beta = 2.0$ and ending at $\beta = 5.0$.
% %
% Additionally, the training was distributed across $16$ nodes on COOLEY using \texttt{horovod}.
% %
% The results are averaged over all $N_{\mathrm{samples}}$ samples in the
% mini-batch.
%
% I'm currently working on re-running all of the following experiments to see if this difference $\plaqdiff$ is
% reproducible or if it is some sort of anomaly.
%
%
\begin{figure}[htpb]\label{fig:plaq_diff_plots_lf5}
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf5_beta5.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf5_beta5_HMC.eps}
  %
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf5_beta6.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf5_beta6_HMC.eps}
  \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 5$ for $\beta = 5.0$ (top row) and $\beta = 6.0$ (bottom
    row). The results from the trained L2HMC (generic HMC) sampler are shown in the left (right) column. As can be seen,
    the difference $\delta_{\phi_{P}}$ remains roughly consistent for all values of $\alpha_Q$.}
\end{figure}
%
\begin{figure}[htpb]\label{fig:plaq_diff_plots_lf6}
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf6_beta5.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf6_beta5_HMC.eps}
  %
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf6_beta6.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf6_beta6_HMC.eps}
  \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 5$ for $\beta = 5.0$ (top row) and $\beta = 6.0$ (bottom
    row). The results from the trained L2HMC (generic HMC) sampler are shown in the left (right) column. As can be seen,
    the difference $\delta_{\phi_{P}}$ remains roughly consistent for all values of $\alpha_Q$.}
\end{figure}
%
\begin{figure}[htpb]\label{fig:plaq_diff_plots_lf7}
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf7_beta5.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf7_beta5_HMC.eps}
  %
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf7_beta6.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf7_beta6_HMC.eps}
  \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 7$ As can be seen, the difference $\delta_{\phi_{P}}$ is
    noticeably larger for $\alpha_Q = 0.75$, but remains roughly consistent for all other values of $\alpha_Q$.}
\end{figure}
%
\begin{figure}[htpb]\label{fig:plaq_diff_plots_lf8_beta5}
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf8_beta5.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf8_beta5_HMC.eps}
  %
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf8_beta6.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf8_beta6_HMC.eps}
  \caption{$\plaqdiff$ vs. MD step with $N_{\mathrm{LF}} = 8$. As can be seen, the difference $\delta_{\phi_{P}}$
    remains roughly consistent for all values of $\alpha_Q$.}
\end{figure}
%
\begin{figure}[htpb]\label{fig:plaq_diff_plots_lf9}
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf9_beta5.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf9_beta5_HMC.eps}
  %
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf9_beta6.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf9_beta6_HMC.eps}
  \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 9$. As can be seen, the difference $\delta_{\phi_{P}}$ is
  largest for $\alpha_Q = 0.0$ and $\alpha_Q = 0.5$.}
\end{figure}
%
\begin{figure}[htpb]\label{fig:plaq_diff_plots_lf10}
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf10_beta5.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf10_beta5_HMC.eps}
  %
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf10_beta6.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf10_beta6_HMC.eps}
  \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 10$. As can be seen, the difference $\delta_{\phi_{P}}$ is
    smallest for $\alpha_Q = 0.,\,\,0.75$ and largest for $\alpha_Q = 0.25$, while $\alpha_Q = 0.5,\,\,2.0$, takes on
  intermediate values.}
\end{figure}
%
\begin{figure}[htpb]\label{fig:plaq_diff_plots_lf15}
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf15_beta5.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf15_beta5_HMC.eps}
  %
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf15_beta6.eps}
  \hfill
  \includegraphics[width=0.49\textwidth]{plaq_difference/plaq_diff_lf15_beta6_HMC.eps}
  \caption{$\plaqdiff$ vs MD step with $N_{\mathrm{LF}} = 15$. As can be seen, the difference $\delta_{\phi_{P}}$
    varies for different values of $\alpha_Q$.}
\end{figure}
%
% \include{autocorrelation_plots}
%
\section{UPDATES (05/15/2019) Debugging convergence issues}
We mainly want to know:
\begin{itemize}
  \item How the plain link variables, $\phi_{\mu}(i) \in [-\pi, \pi)$ change during the leapfrog updates. If we
    denote by $\phi_{\mu}^{(t)}{(i)}$ an individual link variable at the $t^{th}$ leapfrog step then we can look at how
    these values change between subsequent leapfrog updates through the quantity
    \begin{equation}
      \delta \phi_{\mu}{(i)} = \phi_{\mu}^{(t+1)}{(i)} - \phi_{\mu}^{(t)}{(i)}
    \end{equation}
    For illustrative purposes, we can look at how the average link changes and define
    \begin{align}
      \langle \delta \phi_{\mu}{(i)}\rangle &= \frac{1}{N_{\mathrm{links}}}\sum_{\mu, i} \delta \phi_{\mu}(i) \\
    \end{align}
  %
  \item How the determinant of the Jacobian changes during the leapfrog updates. For the augmented leapfrog operator
    $\lfop$, we can compute the Jacobian $\mathcal{J}$:
    \begin{align}
      \mathcal{J} &\equiv \log\left|\frac{\partial {[\mathbf{F}\lfop\xi]}}{\partial\xi^{T}}\right| = \sum_{t \leq
                      N_{\mathrm{lf}}} \mathcal{J}^{(t)} \\
                  &= \sum_{t\leq N_{\mathrm{lf}}} \log\left|\frac{\partial 
                      {[\mathbf{F}\lfop\xi^{t}]}}{\partial\xi^{T}}\right| \\
                  &= d \, \sum_{t \leq M} \bigg[\frac{\eps}{2}\mathbbm{1}\cdot S_{v}{(\zeta_1^t)}
                      + \varepsilon m^t \cdot S_x{(\zeta_2^t)} + \varepsilon \bar{m}^t \cdot S_x{(\zeta_3^t)}
                      + \frac{\varepsilon}{2}\mathbbm{1}\cdot S_{v}{(\zeta_4^t)}\bigg]
    \label{eq:logdet}
    \end{align}
    where $\zeta_i^t$ denotes the intermediary variable $\zeta_i$ at time step $t$ and $d$ is the direction of $\xi$,
    i.e.\ $\xi = \{x, v, d\}$.
\end{itemize}

Note that in each of the plots below, the faint red (blue) lines represent the value for individual samples in our
mini-batch, whereas the bold red (blue) line indicates the average across all samples.
%
Additionally, for notational simplicity, we define a single MD step to consist of $N_{\mathrm{lf}}$ leapfrog steps
followed by a Metropolis-Hastings accept/reject.

% \include{leapfrog_plots}

\end{document}

