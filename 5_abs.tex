\begin{doublespace}
\begin{tightcenter}
ABSTRACT
\mylinespacing%
\end{tightcenter}

% Prior to your first thesis deposit, replace this text with the text of your scientific/ scholarly abstract. The text
% of this abstract should be double spaced and each new paragraph should be indented. This text may be altered between
% first and final deposits.

%
% This thesis focuses primarily on two main applications of ideas from machine learning and data science to lattice
% simulations.
% We begin with a brief overview of selected topics in machine learning for those who may be unfamiliar, and provide a
% simple example that helps to show how these ideas are carried out in practice.
In this work we explore how lattice gauge theory stands to benefit from new developments in machine learning, and look
at two specific examples that illustrate this point.
%
We begin with a brief overview of selected topics in machine learning for those who may be unfamiliar, and provide a
simple example that helps to show how these ideas are carried out in practice.

% Using the example of configurations generated with the worm algorithm for the
% two-dimensional Ising model, we propose  renormalization group (RG)
% transformations, inspired by the tensor RG, that can be applied to sets of images. We relate criticality to
% the logarithmic divergence of the largest principal component.  We discuss the changes
% in link occupation  under the RG transformation, suggest ways to obtain data
% collapse, and compare with the two state tensor RG approximation near the fixed
% point.

After providing the relevant background information, we then introduce an example of renormalization group (RG)
transformations, inspired by the tensor RG, that can be used for arbitrary image sets, and look at applying this idea
to equilibrium configurations of the two-dimensional Ising model.
% generated using the worm algorithm and represented as
% grayscale images.
% %
% We first relate criticality to the logarithmic divergence of the largest principal component, and introduce an
% analytical framework that is used to justify this relationship.
% %
% We discuss the changes in link occupation under the RG transformation, suggest ways to obtain data collapse, and
% compare with the two-state tensor RG approximation near the fixed point.

%
% From this, we discover that the phase transformation can be characterized by the eigenvalue of the first principal
% component, and construct an analytical framework that justifies this relationship.
%
The second main idea presented in this thesis involves using machine learning to improve the efficiency of Markov Chain
Monte Carlo (MCMC) methods.
%
Explicitly, we describe a new technique for performing Hamiltonian Monte Carlo (HMC) simulations using an alternative
leapfrog integrator that is parameterized by weights in a neural network.
%
This work is based on the L2HMC (`Learning to Hamiltonian Monte Carlo') algorithm introduced
in~\cite{2017arXiv171109268L}.
%
% We first look at applying this technique to a two-dimensional Gaussian Mixture Model and demonstrate a remarkable
% improvement in performance compared to the traditional HMC.
% %
% Motivated by this success, we then look at applying this idea to the two-dimensional $U(1)$ lattice gauge theory.
% %
% In order to more accurately capture the underlying geometry of this model, significant modifications to the original
% algorithm were implemented, and the results from applying this modified implementation are then presented.
% %
% Finally, we discuss ideas for future research and briefly describe how physics as a whole stands to benefit from
% machine learning going forward.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% We begin with a detailed overview of MCMC methods before introducing the HMC algorithm as a tool for sampling from a
% desired target distribution
%
% This thesis discusses new approaches to traditional problems in lattice gauge theory
% developments in machine learning.
%
% We begin with a brief overview of selected topics in machine learning and introduce some relevant notation.
% We begin with a brief overview of selected topics in machine learning that are of particular relevance for our
% discussion
%

% Hamiltonian Monte Carlo algorithm, the most common method of generating gauge configurations for lattice gauge theory
% and lattice QCD.



% L2HMC ABSTRACT {{{
% \begin{abstract}
%     We describe a new technique for performing Hamiltonian Monte-Carlo (HMC)
%     simulations using an alternative leapfrog integrator that is parameterized
%     by weights in a neural network.
%     %
%     We look at applying this technique to a
%     two-dimensional Gaussian Mixture Model and a two-dimensional $U (1) $
%     lattice gauge theory, and compare our results against traditional HMC\@.
%     Ongoing issues and potential areas for improvement are discussed,
%     particularly within the context of HPC and long-term goals of the lattice
%     QCD community.
%
%     % This technique is applied to a
%     % two-dimensional Gaussian Mixture Model and a two-dimensional $U(1)$
%     % lattice gauge theory, where results are compared against traditional HMC.
%     % This algorithm is called L2HMC (for `Learning To Hamiltonian Monte
%     % Carlo`)~\cite{2017arXiv171109268L}.
%
%     % demonstrated for the case of the two-dimensional Gaussian Mixture Model,
%     % This technique is then applied to the 2D $U(1)$ lattice gauge theory model.
%     % Results are then compared to those obtained from generic HMC.
%     % This algorithm is called L2HMC (for `Learning To Hamiltonian Monte
%     % Carlo`)~\cite{2017arXiv171109268L}.
% \end{abstract}
% }}}

% ISING_WORMS ABSTRACT {{{
% Using the example of configurations generated with the worm algorithm for the
% two-dimensional Ising model, we propose  renormalization group (RG)
% transformations, inspired by the tensor RG, that can be applied to sets of images. We relate criticality to
% the logarithmic divergence of the largest principal component.  We discuss the changes
% in link occupation  under the RG transformation, suggest ways to obtain data
% collapse, and compare with the two state tensor RG approximation near the fixed
% point.
% }}}
%

% Benefits and common issues encountered when using this approach are also discussed, in
% Common issues encountered when using this algorithm are also discussed before introducing an alternative
% approach called Hamiltonian (Hybrid) Monte Carlo (HMC).
% Hamiltonian (Hybrid) Monte Carlo
% (HMC) methods and describe some of the current issues it faces for certain
% types of problems, and how we believe some of these these difficulties can be
% overcome using a new technique called L2HMC ('learning to Hamiltonian Monte
% Carlo').

\mylinespacing%
% \mylinespacing%
% \begin{tightcenter}
% \textbf{This abstract is required for everyone except DMA and MFA students.}
% \end{tightcenter}
\end{doublespace}
